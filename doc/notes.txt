-------------------------------------------------------------------------------
创建一个三节点的pgsql集群sts
-------------------------------------------------------------------------------

先手工创建三个一样的pv
然后创建sts(要有数据库的用户名和密码)
sts的vct的三个属性要和pv一致
accessModes: [ "ReadWriteOnce" ]
storageClassName: "local-storage"
storage: 1Gi

如果pod长时间pending,说明有错
k describe pod xxx
如果pod的状态是error
k logs pod xxx

k get pod
k get sts
k get pv
k get pvc

k delete pod xxx


删除sts会自动删除pod,但是pvc和pv不会删除
首先删除pvc,然后pv的状态变成released,手动删除


Warning  FailedScheduling  14s   default-scheduler  0/2 nodes are available: 2 pod has unbound immediate PersistentVolumeClaims.
但找不到满足要求的 PV

Warning  FailedScheduling  42s   default-scheduler  0/2 nodes are available: 2 node(s) had volume node affinity conflict.
删除pv的字段nodeAffinity

-------------------------------------------------------------------------------
进入其中某个pod, 在里面创建一个db, 一个表, 做一次查询
-------------------------------------------------------------------------------

我已经成功创建了pgsql sts, 我现在想进入其中某个pod, 在里面创建一个db, 一个表, 做一次查询
ChatGPT
首先，你需要使用 kubectl exec 命令进入你想要操作的 Pod。Pod 的名字会以 StatefulSet 名字为基础，再加上序号。例如，如果你的 StatefulSet 名字是 postgresql，那么 Pod 的名字会是 postgresql-0、postgresql-1、postgresql-2 等等。以 postgresql-0 为例，你可以使用以下命令进入这个 Pod：

bash
Copy code
kubectl exec -it postgresql-0 -- /bin/bash
这条命令会在 Pod 内启动一个 bash shell，你可以在这个 shell 中执行命令。你应该会看到命令提示符变成了 root@postgresql-0:/#，表示你现在在 Pod 内部。

然后，你可以运行 psql 命令连接到 PostgreSQL 数据库：

bash
Copy code
psql -U postgres
这条命令会连接到数据库，你应该会看到命令提示符变成了 postgres=#，表示你现在已经连接到数据库。

然后，你可以运行 SQL 命令创建一个数据库：

sql
Copy code
CREATE DATABASE mydatabase;
然后，你可以切换到这个数据库：

sql
Copy code
\c mydatabase
然后，你可以创建一个表：

sql
Copy code
CREATE TABLE mytable (
    id serial PRIMARY KEY,
    name VARCHAR (50) NOT NULL
);
然后，你可以插入一些数据：

sql
Copy code
INSERT INTO mytable (name) VALUES ('name1'), ('name2'), ('name3');
最后，你可以查询这个表：

sql
Copy code
SELECT * FROM mytable;
当你完成所有操作后，你可以输入 \q 退出 psql，然后输入 exit 退出 Pod。

-------------------------------------------------------------------------------
创建headless service
-------------------------------------------------------------------------------
查看发行版和内核版本
uname -r
cat /etc/*-release
安装ping
apt-get update
apt-get install -y iputils-ping


kubectl get pods -o wide
kubectl get svc
kubectl config get-contexts #查看命名空间
Pod的DNS名字的格式为pod-name.service-name.namespace-name.svc.cluster.local

切换命名空间
kubectl config current-context
kubectl get namespaces
kubectl config set-context --current --namespace=<目标命名空间>
kubectl config view --minify | grep namespace:
kubectl config set-context --current --namespace=<目标命名空间>

执行下面的命令，这将打开一个busybox的shell：
kubectl run -it --rm --restart=Never busybox --image=busybox sh
在这个shell中，你可以使用 nslookup 或 dig 命令来查询Kubernetes的服务名字，例如：
nslookup kubernetes.default.svc.cluster.local
如果DNS正常工作，你将看到对应的IP地址。
当你完成测试后，你可以使用 exit 命令退出busybox的shell，这将自动删除这个Pod。

必须先启动headless-service.yaml
再启动sts.yaml
sts里不但要配置label,还有一个最重要的配置serviceName
  selector:
    matchLabels:
      app: postgres
  serviceName: "pgsql-headless"
  template:
之后可以到一个pod里来ping其他的pod的DNS
apt-get install -y iputils-ping
apt-get install -y dnsutils

I encountered a problem that each db pod cant visit each others's DNS name.
It turned out that I set the parameters in the headless-service wrongly. 
So if you get the same problems again, please follow the exact steps in define headless-service and the start sequence.
Espicially the names in service.

-------------------------------------------------------------------------------
同步复制
-------------------------------------------------------------------------------

完整的同步复制方案。以下是一个更为详细的步骤，这个方案是基于你已经创建了 PostgreSQL StatefulSet pgset：

在主数据库上（pgset-0）

进入pgset-0 pod：

shell
Copy code
kubectl exec -it pgset-0 -- /bin/bash
在 pgset-0 上创建一个新用户，用于数据复制：

shell
Copy code
psql -U postgres -c "CREATE USER repl REPLICATION LOGIN CONNECTION LIMIT 1 ENCRYPTED PASSWORD 'replpassword';"
编辑 PostgreSQL 的配置文件 postgresql.conf 以及 pg_hba.conf。在 Kubernetes 环境中，这些文件通常位于 /var/lib/postgresql/data/：

在 postgresql.conf 中，添加/修改以下行：

shell
Copy code
wal_level = replica
max_wal_senders = 3
max_wal_size = 1GB
synchronous_standby_names = 'pgset-*'  # '*' allows any standby to sync
在 pg_hba.conf 文件中，添加以下行来允许复制用户连接：

shell
Copy code
host    replication     repl      0.0.0.0/0               md5
保存并退出。

重启 pgset-0 实例，以便配置更改生效。你可以通过 Kubernetes 的删除 pod 的方式重启它。因为你使用了 StatefulSet，因此 Kubernetes 将自动重新创建 pod。

-------------------------------------------------------------------------------
改用12
-------------------------------------------------------------------------------
如果整个程序以pg 15启动后
即使把K8S资源都清空了,pv的数据还是在硬盘上
此时改用pg 12启动, 会读取上次的pv
找到pv的存储路径, 清空了,还是不行
只有改名,把pv的宿主路径名改了, 才启动成功
可见pv的数据是保存在其他地方的, 删不掉

pod连接不上nslookup kubernetes.default.svc.cluster.local
原因是复制虚拟机副本时, 没有生成了新的MAC地址, 正确的方法应该是"包含所有网卡的MAC地址"
我是怎么解决的? 我用ifconfig查看了IP地址, 和配置说明书上的一样, 这时我想到可能是复制完副本的启动顺序有问题.
我发现我启动新的副本时, 那个坏掉的副本没关掉, 我以为导致重新分配了IP, 所以决定重新复制
我想重新复制, 重新复制时发现MAC的复制策略有问题, 原来是上次选错了



登录到Pod里面：你首先需要找到你的Pod的名称。你可以使用kubectl get pods命令查看你的Pod的名称，然后你可以通过kubectl exec -it <pod-name> -- /bin/bash命令登录到Pod里面。在这里，<pod-name>需要替换成你的Pod的名称。
(这两个操作都只用在主节点中做)
修改listen_addresses：在Pod里面，你可以使用psql命令进入PostgreSQL。然后，你可以使用ALTER SYSTEM SET listen_addresses TO '*';命令来修改listen_addresses设置。这样，PostgreSQL将监听所有的IP地址。然后，你需要通过SELECT pg_reload_conf();命令重载配置，使修改生效。

创建新用户：在PostgreSQL中，你可以通过以下命令创建一个新的用户：CREATE USER replicarole WITH REPLICATION ENCRYPTED PASSWORD 'SuperSecret';。




pg_basebackup -h pgset-0.pgsql-headless.default.svc.cluster.local -U replicarole -p 5432 -D /var/lib/PostgreSQL/12/main -Fp -Xs -P -R -S master

-------------------------------------------------------------------------------
配置复制集群的方法
-------------------------------------------------------------------------------

kubectl exec -it pgset-0 -- bash
psql -U postgres -c "ALTER SYSTEM SET listen_addresses TO '*';"
cat /var/lib/postgresql/data/pgdata/postgresql.conf | grep listen_addresses
psql -U postgres -c "SELECT pg_reload_conf();"
psql -U postgres -c "CREATE USER replicarole WITH REPLICATION ENCRYPTED PASSWORD 'SuperSecret'"
SELECT * FROM pg_roles WHERE rolname='replicarole';
#只需要在主服务器上设置listen_address和创建replicarole用户。从服务器不需要这些操作，因为它们只需要从主服务器接收和应用WAL日志，不需要接受来自其他服务器的连接，也不需要replicarole用户进行复制。

/var/lib/postgresql/data/pgdata/pg_hba.conf
应该允许来自任何 IP 地址的 replicarole 用户进行复制。
所以你应该将 pg_hba.conf 配置文件中在文件中添加以下行
host replication replicarole all md5
运行 pg_reload_conf 函数重新加载配置：
psql -U postgres -c "SELECT pg_reload_conf();"

解读WAL和Slot
什么是WAL段：
WAL是PostgreSQL的写前日志（Write-Ahead Logging）的缩写。当你在数据库中执行一些修改操作（如插入、更新或删除）时，PostgreSQL会首先把这些修改记录到WAL中，然后再将这些修改应用到实际的数据文件中。这样做的好处是，如果在修改应用到数据文件之前数据库发生了崩溃，那么在数据库重启后可以通过重播WAL中的记录来恢复数据库的状态。
一个WAL段是WAL的一个物理存储单位。在PostgreSQL中，WAL被分割成一系列连续的段，每个段的大小默认为16MB。

在一个PostgreSQL的主从复制环境中，主服务器会将WAL段发送到从服务器，从服务器则会重播这些WAL段来保持与主服务器的数据同步。如果从服务器由于某些原因停机，然后在一段时间后重启，那么在这段时间内主服务器可能会生成新的WAL段，并删除旧的WAL段。这样的话，从服务器在重启后可能会发现它需要的WAL段已经被主服务器删除了，这样从服务器就无法与主服务器保持同步。
为了解决这个问题，通过使用Slot技术，主服务器可以动态地保留需要的WAL段。你可以在主服务器上为每个从服务器创建一个Slot，主服务器会记录每个Slot所需的最旧的WAL段，然后保留所有从这个WAL段开始及其后面的WAL段。这样的话，每个从服务器重启后都能从主服务器获取到自己需要的WAL段。这种方案可以更有效地使用磁盘空间。

SELECT * FROM pg_create_physical_replication_slot('pgset1_slot');
SELECT pg_drop_replication_slot('pgset1_slot');

对于pg_basebackup命令，这是一个用来创建PostgreSQL数据库的基础备份的工具，它可以同时创建一个完整的数据库备份和所有相关的WAL日志。在初始化主从复制时，pg_basebackup常被用来在从服务器上创建一个与主服务器完全相同的数据库副本。尽管你现在的目标可能只是复制数据库，但是使用pg_basebackup工具可以使得从服务器上的数据库与主服务器保持完全一致，并且能够自动处理一些复制过程中可能出现的问题，如处理WAL日志的复制等。因此，我会推荐你在初始化主从复制时使用pg_basebackup工具。在你的Kubernetes环境中，你可能需要创建一个init-container来执行pg_basebackup命令。

max_wal_senders参数，是用来指定主服务器最多可以同时向多少个从服务器发送WAL日志。如果你的环境中只有一个从服务器，那么你可以将max_wal_senders设置为1。但是如果你计划将来可能会有更多的从服务器，或者你打算同时使用pg_basebackup工具，那么你可能需要将max_wal_senders设置得更大一些。这个参数也是在主服务器的postgresql.conf文件中设置的。

现在配置从节点
在从服务器上，我们需要关闭PostgreSQL服务，删除PGDATA目录，然后重新创建一个空的、具有正确权限的目录。我们可以使用以下命令来实现：这个步骤实际上是在准备 pgset-1 作为复制的从服务器，使其可以接收 pgset-0 的数据。你需要根据你的实际情况判断这个步骤是否必要，因为这会清空 pgset-1 上的所有数据。为了安全起见，你需要先确认 PostgreSQL 服务已停止。你可以使用 psql 命令尝试连接到数据库。如果连接失败，那么 PostgreSQL 服务应该已经停止。在 Kubernetes 中，你可能无法直接停止和启动 PostgreSQL 服务。相反，你可能需要删除并重新创建 pod 来达到相同的目的。这可能需要你停止 StatefulSet，然后手动删除 pod。
kubectl exec -it pgset-1 -- bash
rm -rf /var/lib/postgresql/data/pgdata
mkdir /var/lib/postgresql/data/pgdata
chown postgres:postgres /var/lib/postgresql/data/pgdata
chmod 0700 /var/lib/postgresql/data/pgdata

kubectl exec -it pgset-1 -- bash
创建standby.signal文件。它是一个空文件，用于通知PostgreSQL在启动时运行在standby（或从）模式下。
touch /var/lib/postgresql/data/pgdata/standby.signal

修改postgresql.auto.conf文件以添加主服务器连接设置。在这里，你需要添加两行来设置primary_conninfo和primary_slot_name参数：
echo "primary_conninfo = 'user=replicarole password=SuperSecret host=pgset-0.pgsql-headless.default.svc.cluster.local port=5432 sslmode=prefer sslcompression=0 gssencmode=prefer krbsrvname=postgres target_session_attrs=any'" >> /var/lib/postgresql/data/pgdata/postgresql.auto.conf
echo "primary_slot_name = 'master'" >> /var/lib/postgresql/data/pgdata/postgresql.auto.conf

现在让我们使用正确的选项运行pg_basebackup命令。该命令将执行从主机到从机的base_backup命令，并准备从机接收和执行接收到的WAL段，使从服务器保持在持续恢复的状态：
pg_basebackup用于从运行中的PostgreSQL数据库服务器创建一个基础备份。在设置主从复制的过程中，pg_basebackup通常用于在从服务器上创建一个主服务器的一致性快照，以初始化从服务器。
pg_basebackup命令的一些选项如下：
-h 主机名，这里是主服务器的地址；
-U 用户名，这里是复制角色；
-p 端口，这里是主服务器的端口；
-D 目标目录，这里是从服务器的数据目录；
-Fp 输出文件格式，这里是普通文件；
-Xs 包含WAL文件的方式，这里是流式传输；
-P 显示进度；
-R 创建恢复配置文件，也就是在从服务器的数据目录中创建recovery.conf文件；
-S 复制槽的名称。
kubectl exec -it pgset-1 -- su - postgres -c "pg_basebackup -h pgset-0.pgsql-headless.default.svc.cluster.local -U replicarole -p 5432 -D /var/lib/postgresql/data/pgdata -Fp -Xs -P -R -S pgset1_slot"

我目前的思路是, 从db pod启动时, 先启动一个init-container,做以下初始化
standby.signal
postgresql.auto.conf
pg_basebackup
然后检查日志k logs pgset-1
不要报这个错2023-06-03 23:48:01.962 UTC [33] FATAL:  database system identifier differs between the primary and standby

-------------------------------------------------------------------------------
启动流程梳理
-------------------------------------------------------------------------------
主节点启动
主节点启动后
psql -U postgres -c "ALTER SYSTEM SET listen_addresses TO '*';"
psql -U postgres -c "SELECT pg_reload_conf();"
psql -U postgres -c "CREATE USER replicarole WITH REPLICATION ENCRYPTED PASSWORD 'SuperSecret'" (IF NOT EXISTS)
echo "host replication replicarole all md5" >> /var/lib/postgresql/data/pgdata/pg_hba.conf (IF NOT EXISTS)
#!/bin/bash
PGDATA_PATH="/var/lib/postgresql/data/pgdata"
if [ -f "${PGDATA_PATH}/postgresql.conf" ]; then
  # 使用 sed 命令来修改 max_wal_senders 的值
  sed -i "s/^#*max_wal_senders =.*$/max_wal_senders = 2/" "${PGDATA_PATH}/postgresql.conf"
  echo "max_wal_senders updated successfully in ${PGDATA_PATH}/postgresql.conf"
else
  echo "postgresql.conf file not found in ${PGDATA_PATH}"
fi
psql -U postgres -c "SELECT pg_reload_conf();"
psql -U postgres -c "SELECT * FROM pg_create_physical_replication_slot('pgset1_slot');" (IF NOT EXISTS)
从节点启动
从节点启动后
touch /var/lib/postgresql/data/pgdata/standby.signal (IF NOT EXISTS)
echo "primary_conninfo = 'user=replicarole password=SuperSecret host=pgset-0.pgsql-headless.default.svc.cluster.local port=5432 sslmode=prefer sslcompression=0 gssencmode=prefer krbsrvname=postgres target_session_attrs=any'" >> /var/lib/postgresql/data/pgdata/postgresql.auto.conf (IF NOT EXISTS)
echo "primary_slot_name = 'pgset1_slot'" >> /var/lib/postgresql/data/pgdata/postgresql.auto.conf (IF NOT EXISTS)
kubectl exec -it pgset-1 -- su - postgres -c "pg_basebackup -h pgset-0.pgsql-headless.default.svc.cluster.local -U replicarole -p 5432 -D /var/lib/postgresql/data/pgdata -Fp -Xs -P -R -S pgset1_slot"
重新启动该从节点
成功

-------------------------------------------------------------------------------
异步复制成功, 解决方案
-------------------------------------------------------------------------------
上面的思路是对的, 但是执行的过程出了很大问题
statefulset启动pod有两个钩子
一个是init-container, 一个是poststart
init-container是在主容器启动前运行, 一般用来给这个pvc做一些初始化的工作, 完成后才启动主容器
poststart钩子是在主容器启动后执行, 但是不是在主容器启动完成后执行, 所以具有不确定性
最开始的思路是, 把步骤里的配置工作根据其顺序, 分散到主库和从库的init和poststart里
这就需要增加很多if判断, 判断是不是主节点.这个statefulset越写越复杂.
后来发现, pg_basebackup命令才是重中之重, 我以前以为这个命令只是用来备份恢复的,
其实不然, 它是用来让从库和主库对齐的, 也就是说, 从库必须首先关闭, 然后删掉pgdata所有数据, 然后执行pg_basebackup让从库完全复制主库. 这个完成之后, 从库再启动, 就能接收主库的同步了.
但是k8s有一个问题, 运行数据库的容器, 你登录进去以后, 没法关闭数据库, 因为数据库是该容器的主进程, 重启数据库, 或者关掉数据库, 这个容器就结束了, 所以想关闭数据库, 必须关闭容器.而关闭容器, sts会自动重启容器, 所以就要关闭sts(删除或者设置replica为0), 但是关闭了sts后, 主库也关闭了, 主库关闭后, 不能在从库的pv上执行pg_basebackup命令, 因为该命令需要访问主库获取WAL. 这就陷入了矛盾.
这时候gpt4难得显示出创造力, 他告诉我可以把sts拆成两个, 一个管主库, 一个管从库, 这样, 关了从库的sts也不影响主库的运行,真是绝妙的方法.
这么做之后, 我的方案是, 先把两个sts启动,再把从库sts关掉,此时从库也关掉了,然后用一个pod去访问从库的pv,在这个pod里去做pg_basebackup,对齐主库,然后退出pod,重启从库sts,就成功了.
但是这么做之后, 重启了从库, 一直报一个错
FATAL:  database system identifier differs between the primary and standby
这个错的意思是, 从库不是从主库完全复制过来的副本, 启动了从库后, 没法流式复制数据.
我检查了pg_basebackup的各项参数, 以及主库从库的各项配置, 都没发现问题, 一筹莫展.卡了几个小时.
突然我想到了一个问题, 如何在执行完pg_basebackup, 重启从库之前, 先来验证pg_basebackup有没有成功. 问了gpt4他让我检查pgdata里的几个文件的内容.
一检查出问题了, 我发现pgdata被清空又从pg_basebackup复制之后, 竟然有standby.signal这个文件.这个文件是以前专门在从库上创建的,即使从主库复制过来, 也不可能有这个文件.一定是pv出了问题.
我重启了一个pod访问pvc,发现挂载的pvc竟然是空的, 里面根本没有从库的数据,说明从库的pv根本没有挂载上.我一直在执行pg_basebackup,却没想到根据就没执行到从库的pv上,难怪重启从库后说没有对齐.
pv的问题之前讨论过.我即使在硬盘上删掉了pv,重新安装sts后,还是会访问到老的pv的配置文件,导致数据库没法初始化.我的临时解决方案,是每次重新安装sts,都去声明新的pv,用新的宿主文件夹路径名,这样可以解决,但只是在敷衍,该问题至今无解.
gpt4告诉我,每个pod的pv, 对应的宿主路径名应该不同, 我以前设的都是相同的,两个pv对应了两个hostpath路径.是错的.于是我改了, 但没验证改了后能否解决上面的那个无解问题.
这时我正纠结如何在临时pod上正确挂载pvc,突然我灵机一动,想到,我为什么要单独弄一个pod来做这件事呢?我在从库pod的init-container上来做这件事不是更好吗, 因为init-container本来就要和从库公用一个pvc.
于是我想到出了以下流程: 从库init-container先判断standby.signal是否存在,如果不存在,说明从库还没初始化过,此时不要执行init-container,先跳过,让从库去初始化,同时增加一些复制的配置项.如果该文件已经存在,说明从库已经初始化过了,安装好了,启动过了,就可以执行pg_basebackup来对齐了.原理很简单,如果从库一次都没启动过,从库的pv就是空的,连pg_basebackup这个工具都不存在.所以必须先判断从库是否已经启动过.如果该文件已经存在,则init-container删除从库的所有pgdata数据文件,这时执行pg_basebackup的前置要求,然后执行pg_basebackup对齐.对齐后,init-container结束,从库容器开始运行,因为已经对齐了,所以从库容器自然就进入了等待从主库接收WAL的状态.
至此,问题解决.
卡了这么久,主要就是没有验证pod有没有挂载成功从库的pvc.这一步不验证, 去查后面的问题,就是空中楼阁.因此一个经验是,每一步都要验证成功了没有,否则后面再出现错误,逻辑链就断了.就是说,有一步出现错误, 比如没对齐.这一步的前置条件是配置项都正确,同时pg_basebackup也正确执行.我想当然认为pg_basebackup已经正确执行, 一直盯着配置项,找错地方了.pg_basebackup没有正确执行,因为pg_basebackup要修改的pv根本就没挂载上.所以后续开发,每一步都要验证了,正确了,再往下走, 不要留隐患.

---------------------
安装kopf
---------------------
sudo apt-get install python3-venv
python3 -m venv venv
source venv/bin/activate
deactivate

pip install -r requirements.txt

pip install kopf==1.35.6
因为这个版本支持Kubernetes 1.23 & 1.24 added to CI
https://github.com/nolar/kopf/releases
而1.36支持更高的版本, 可能会出问题

pip install pykube-ng
https://codeberg.org/hjacobs/pykube-ng
https://pypi.org/project/pykube-ng/22.9.0/#history
(venv-kpof) w@kw-master:~/vpok/kpof-study$ pip list
Package            Version
------------------ --------
aiohttp            3.8.4
aiosignal          1.3.1
async-timeout      4.0.2
attrs              23.1.0
certifi            2023.5.7
charset-normalizer 3.1.0
click              8.1.3
frozenlist         1.3.3
idna               3.4
iso8601            2.0.0
kopf               1.35.6
multidict          6.0.4
pip                22.0.2
pykube-ng          22.9.0
python-json-logger 2.0.7
PyYAML             6.0
requests           2.31.0
setuptools         59.6.0
typing_extensions  4.6.3
urllib3            2.0.3
yarl               1.9.2

如何启动
首先定义一个crd.yaml,应用到k8s上
然后定义一个opt.py,启动,开始监听任何有关crd的事件
最后定义一个cr.yaml,是crd的实例,应用到k8s上,相当于创建了一个资源
opt.py监听到资源创建,create回调函数被调用,执行具体的资源创建工作
就是按顺序应用那一组yaml文件,pv svc sts,并让这些资源归属于该cr
opt.py监听该cr下属的pods,
如果遇到replica-pod进入了running状态,就重启该pod,完成集群初始化
当用户删除该cr时,opt.py执行具体的删除操作,依次删除spok-cr,sts

具体要看代码, 最难的一点是如何在该opt的生命周期中保存状态,因为replica-pod只能执行
https://kopf.readthedocs.io/en/stable/memos/
在这里找到解决方案,具体是要使用Custom memo classes

启动opt的常规方法是
kopf run spok_opt.py --verbose
但是因为定义了状态,所以要用opt的main函数启动

启动步骤
启动opt
实例化crd
验证
kubectl exec -it pgset-master-0 -- /bin/bash
kubectl exec -it pgset-replica-0  -- /bin/bash
psql -h localhost -p 9999 -U postgres

1. 设置一个secret保存psql的用户名和密码
2. 设置一个configmap保存pgpool2的启动参数，尤其是设置好两个数据库的域名
3. 启动一个deployment部署pgpool2，要用到secret和configmap，不需要使用pcp
4. 登陆pgpool2 pod然后访问数据库psql -h localhost -p 9999 -U postgres

-------------------------------------------------------------------------------
synchronous replication
-------------------------------------------------------------------------------
要验证 PostgreSQL 设置为同步复制，你可以使用以下方法：

查询 pg_stat_replication 视图： pg_stat_replication 是一个视图，包含了关于每个复制工作的状态信息。你可以执行如下查询来查看复制状态：

sql
Copy code
SELECT * FROM pg_stat_replication;
在查询结果中，state 列的值应为 'streaming'，表示主服务器和副服务器间的复制正在进行。sync_state 列的值为 'sync'，表示副服务器正在与主服务器进行同步复制。
-------------------------------------------------------------------------------
PV
-------------------------------------------------------------------------------
使用local时，必须在本机上创建好文件夹，才能挂载到PV上
使用hostpath时，PV自动创建文件夹，但这个文件夹可能位于任何节点，必须手动全部删除，否则下次启动数据库会使用老数据

-------------------------------------------------------------------------------
psql configuration
-------------------------------------------------------------------------------
在 PostgreSQL 中，你可以使用 ALTER SYSTEM 命令来修改配置参数，但这些修改并不直接反映在主配置文件 postgresql.conf 中。相反，它们会写入到一个名为 postgresql.auto.conf 的文件中，这个文件位于你的数据目录下，例如 /var/lib/postgresql/data/pgdata/postgresql.auto.conf。

这是因为 ALTER SYSTEM 是一个会话级别的命令，它对 PostgreSQL 的行为产生影响，但不会直接修改主配置文件。

为了查看 synchronous_standby_names 参数的当前值，你可以在 PostgreSQL 会话中执行以下 SQL 命令：

sql
Copy code
SHOW synchronous_standby_names;
你也可以通过查询 pg_settings 视图来查看所有配置参数的当前值：

sql
Copy code
SELECT name, setting FROM pg_settings WHERE name = 'synchronous_standby_names';
这个视图包含了 PostgreSQL 的所有配置参数，包括那些被 ALTER SYSTEM 修改过的参数。

postgres=# SELECT name, setting FROM pg_settings WHERE name = 'synchronous_standby_names';
           name            | setting 
---------------------------+---------
 synchronous_standby_names | *
(1 row)

------------------------------------------------------------------------------
poststart logs
------------------------------------------------------------------------------
#example-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: echo-demo-pod
spec:
  containers:
  - name: echo-demo-container
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 10; done']
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "mkdir -p /spok_logs && echo 'postStart script running' > /spok_logs/poststart.log"]
#EOF
#show realtime logs
kubectl exec echo-demo-pod -- tail -f /spok_logs/poststart.log
#show logs
kubectl exec echo-demo-pod -- cat /spok_logs/poststart.log

------------------------------------------------------------------------------
manually scaling
------------------------------------------------------------------------------
我想在k8s上搭建一个psql复制集群，psql版本是12.15
psql集群采用一主多从的形式
目前已经实现了一主一从，主从各有一个sts来管理
主从的配置分别写在各自的sts的poststart钩子里
我现在想实现手动伸缩，就是修改了从库的sts的replicas数量之后，可以自动生成对应的从库

1.我有一个crd来管理这些，我修该了cr的standby-replicas数量之后（我只要修改此处的配置）

2.首先master要增加相应的pgset1_slot（这一步可以在我的kopf operator中实现）

3.然后replica要通过poststart修改以下操作的pgset-replica-1和pgset1_slot，这两个值的计数对于第一个从库都是0，之后的从库变成1，2，3等。
PRIMARY_CONNINFO+=" application_name=pgset-replica-1.pgsql-headless.default.svc.cluster.local"
echo "primary_slot_name = 'pgset1_slot'" >> /var/lib/postgresql/data/pgdata/postgresql.auto.conf

第三步比较困难，因为我的poststart脚本都是放在configmap里的，我不想针对每个replica都修改该脚本，最好是该脚本能够读取环境变量，然后使用该环境变量作为这两处的名字。那么就需要Operator在从库sts启动从库前，给从库的启动pod增加环境变量？比如standby-id: 3，然后，从库的poststart脚本就使用pgset-replica-3和pgset3_slot这两个名字。或者我就不用设置环境变量，我直接让Operator读取新的从库Pod的Pod-name，作为从库poststart的一个参数。

我不确定我的方案是否正确，如果正确，你告诉我具体该怎么做。然后再给我一个更好的方案。

解决方案，在sts的init-container和containers里都设置一个env POD_NAME，用fieldPath: metadata.name此方法取到POD名字
然后在init-container和poststart脚本里使用该POD名字来命名对应的SLOT
master默认注册的SLOT是0，如果以后要再添加新的从库，就在Operator里给master create对应的SLOT。但是replica就不用管了，因为replica可以根据POD名找到刚才create的SLOT
但是最好让master poststart里也不用默认生成slot0，让slot0也由postgres生成是最好的。
另外，最好等master可用了再启动slave，但目前还没有做到，应该在operator里轮询检测再启动

---------------------------------------------------------------
对于你的方案，理论上是可行的。让我解释一下应该如何操作：

你的 Operator 应该可以获取到 Pod 的名称，因此，你可以使用这个名称作为环境变量。例如，如果你的 Pod 的名称是 pgset-replica-3，你可以在你的 Operator 中这样设置环境变量：

yaml
Copy code
env:
- name: POD_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.name
这会将 Pod 的名称设置为 POD_NAME 环境变量。

然后，在你的 post-start 钩子中，你可以使用这个环境变量来动态地设置应用名称和复制槽的名称。例如：

bash
Copy code
PRIMARY_CONNINFO+=" application_name=${POD_NAME}.pgsql-headless.default.svc.cluster.local"
echo "primary_slot_name = '${POD_NAME}_slot'" >> /var/lib/postgresql/data/pgdata/postgresql.auto.conf
然后你需要在 master 节点上创建一个新的复制槽。你可以在你的 Operator 中执行一个命令，例如：

bash
Copy code
psql -U postgres -c "SELECT * FROM pg_create_physical_replication_slot('${POD_NAME}_slot');"
这将为每个 replica pod 创建一个唯一的复制槽。

这是一个可能的解决方案，但可能还有其他的方法来解决这个问题。你可能也想看看是否可以使用已经存在的工具或框架来帮助你管理 PostgreSQL 集群，例如 Patroni 或者 Stolon。这些工具已经解决了很多在运行 PostgreSQL 集群时可能会遇到的问题，例如故障转移和配置同步。

User
poststart只能读取环境变量 ，不能直接读取到pod的name?
还有一个重大问题，当我修改了sts的replicas这个参数，比如改为4，那么pod3应该是立即启动的，我哪有机会去先给这个pod增加环境变量？
ChatGPT
对于你的第一个问题，post-start 钩子可以读取环境变量，因为它们是在容器的上下文中运行的。在 Kubernetes 中，Pod 的元数据，包括它的名称，是可以通过环境变量暴露给 Pod 中运行的应用程序的。在我之前给出的例子中，我们使用 fieldRef 来从 Pod 元数据中获取 Pod 的名称，并将其作为环境变量。

对于你的第二个问题，这个环境变量应该在 Pod 创建的时候设置，而不是在 Pod 创建后设置。也就是说，当你修改了 StatefulSet 的 replicas 时，Kubernetes 将会为每个新的 Pod 创建一个新的环境变量，这个环境变量将包含这个 Pod 的名称。这个环境变量的值将会在 Pod 的整个生命周期中保持不变。

这样，你就可以在你的 post-start 钩子中使用这个环境变量了。当 post-start 钩子运行时，这个环境变量已经设置好了，所以你的脚本可以直接使用它。
-------------------------------------------------------------
在Operator中写响应函数时，一直不会调用，后来才发现，原来这个函数写在main函数后面了，所以才没响应，所以回调函数必须写在main函数前面
-------------------------------------------------------------
遇到一个问题，创建主库或者从库经常错误，后来发现是k8s的一个工作节点挂掉了，考虑还是用local pv更稳定

[2023-06-17 18:18:19,362] kopf.objects         [INFO    ] Pod pgset-replica-0开始重启，这是现在的状态{'pgset-replica-0': ReplicaState(has_been_restarted_by_opt=True, is_now_deleted_by_opt=True)}
[2023-06-17 18:18:19,366] kopf.objects         [INFO    ] Handler 'pod_event_fn' succeeded.
[2023-06-17 18:18:19,366] kopf.objects         [DEBUG   ] Handler 'pod_event_fn' is invoked.
[2023-06-17 18:18:19,372] kopf.objects         [INFO    ] Received an event: MODIFIED for pod: pgset-replica-0
[2023-06-17 18:18:19,373] kopf.objects         [INFO    ] Handler 'pod_event_fn' succeeded.
[2023-06-17 18:18:19,476] kopf.objects         [DEBUG   ] Handler 'pod_event_fn' is invoked.
[2023-06-17 18:18:19,476] kopf.objects         [INFO    ] name:pgset-replica-0,                phase:Running,                memo:{'pgset-replica-0': ReplicaState(has_been_restarted_by_opt=True, is_now_deleted_by_opt=True)}
[2023-06-17 18:18:19,479] kopf.objects         [INFO    ] Pod pgset-replica-0已经完成重启，这是现在的状态{'pgset-replica-0': ReplicaState(has_been_restarted_by_opt=True, is_now_deleted_by_opt=False)}
[2023-06-17 18:18:19,480] kopf.objects         [INFO    ] Handler 'pod_event_fn' succeeded.
[2023-06-17 18:18:19,480] kopf.objects         [DEBUG   ] Handler 'pod_event_fn' is invoked.
[2023-06-17 18:18:19,480] kopf.objects         [INFO    ] Received an event: MODIFIED for pod: pgset-replica-0
[2023-06-17 18:18:19,481] kopf.objects         [INFO    ] Handler 'pod_event_fn' succeeded.
[2023-06-17 18:18:21,380] kopf.objects         [DEBUG   ] Handler 'pod_event_fn' is invoked.
[2023-06-17 18:18:21,381] kopf.objects         [INFO    ] name:pgset-replica-0,                phase:Running,                memo:{'pgset-replica-0': ReplicaState(has_been_restarted_by_opt=True, is_now_deleted_by_opt=False)}
[2023-06-17 18:18:21,406] kopf.objects         [INFO    ] Handler 'pod_event_fn' succeeded.
[2023-06-17 18:18:21,408] kopf.objects         [DEBUG   ] Handler 'pod_event_fn' is invoked.
[2023-06-17 18:18:21,409] kopf.objects         [DEBUG   ] Handler 'pod_event_fn' is invoked.
[2023-06-17 18:18:21,419] kopf.objects         [INFO    ] Received an event: DELETED for pod: pgset-replica-0
[2023-06-17 18:18:21,419] kopf.objects         [INFO    ] Pod pgset-replica-0 has been deleted.
[2023-06-17 18:18:21,419] kopf.objects         [INFO    ] name:pgset-replica-0,                phase:Pending,                memo:{'pgset-replica-0': ReplicaState(has_been_restarted_by_opt=True, is_now_deleted_by_opt=False)}
[2023-06-17 18:18:21,420] kopf.objects         [INFO    ] Pod pgset-replica-0遇到了删除事件，这是现在的状态{'pgset-replica-0': ReplicaState(has_been_restarted_by_opt=True, is_now_deleted_by_opt=False)}
[2023-06-17 18:18:21,429] kopf.objects         [INFO    ] 该pod失败，但不是opt导致的，那么要重置该副本的状态

发现了这个问题，当已经完成重启，进入了running状态，那边delete事件才收到，所以它以为不是opt重启的，就导致循环重启

pip install kubernetes

--------------------------------------------------------------
6.18今天继续实现自动伸缩
把scaleout的代码抽取出来成scaleout函数
然后再实现一个scalein函数
scaleout函数首先要去master上注册一个slot，然后修改opt的状态，最后更新sts，就会自动生成pod了，db的初始化工作由pod的钩子处理
scalein函数首先要更新sts，删除相关节点，然后去master上drop对应的slot，最后修改opt的状态（standby计数和每个副本的启动状态）
遇到了两个问题，一是crd变更响应函数的参数status的replica数一直是1，而且无法改变，导致我无法正确计算出还要scaleout几个节点
另一个问题是，我想通过patch的方法来更新crd的replica计数，但是这个patch却不能触发上述的update函数
只能采取第三种方案，就是在opt的状态中保存当前的rep计数，并手动管理，这种方法简单多了
另一个解决方案是：默认必须有一个replica，所以它的初始memo状态开始就设置好。后面需要几个rep就scaleout几个
kopf目前不支持查询crd，因为pykube没有该功能。所以该功能要用k8s api来实现
还要处理on_start恢复memo状态的情况
还要处理能同时启动多个pgsql集群的情况
----------------------------------------------------------------------------------------------------------
web configuration
----------------------------------------------------------------------------------------------------------

我的k8s环境是：windows笔记本，两台virtualbox虚拟机，分别装了ubuntu。我在ubuntu上搭建了kubernetes集群，都是本地的。一共两个节点。我现在需要在这个k8s上启用k8s的web配置管理系统。请告诉我具体可操作的完整步骤。
ChatGPT
Kubernetes的Web用户界面（通常称为Dashboard）是一个通用的基于web的Kubernetes用户界面。您可以在Dashboard中部署容器化的应用，对应用进行故障排除，还可以管理集群资源。这里是启用Kubernetes Dashboard的步骤：

首先，在你的主节点上（任何一个运行kubelet并且已经设置为master的节点），运行以下命令以部署Kubernetes Dashboard：

bash
Copy code
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml
这将在kubernetes-dashboard namespace下部署Kubernetes Dashboard。

接下来，你需要创建一个具有管理员权限的Service Account，以便访问Dashboard。可以创建一个名为admin-user的Service Account，并赋予其cluster-admin的角色。这可以通过以下YAML文件完成：

yaml
Copy code
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
保存上面的内容为dashboard-adminuser.yaml，然后使用kubectl应用它：

bash
Copy code
kubectl apply -f dashboard-adminuser.yaml
然后，你需要获取admin-user Service Account的Bearer Token。使用以下命令：

bash
Copy code
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
这将显示一个token，你需要将此token复制保存，用于登录Dashboard。

现在，你应该启动Kubernetes proxy，以便在本地访问Dashboard：

bash
Copy code
kubectl proxy
然后，在你的浏览器中，你可以通过以下URL访问Dashboard：

bash
Copy code
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
它会提示你输入一个token，将刚才复制的token粘贴到这里，然后点击“SIGN IN”。这时，你应该可以看到Kubernetes Dashboard，并可以开始管理你的Kubernetes集群。

注意：由于默认情况下，kubernetes-dashboard仅接受localhost的连接，因此你需要在你的主节点上执行上述操作。如果你想从远程或者其他节点访问，可能需要对你的网络配置进行一些更改。





User
该界面好像不能显示custom resource definition?
ChatGPT
是的，截至我所知道的最新信息（2021年9月），Kubernetes Dashboard 并不直接支持显示或管理 Custom Resource Definitions (CRDs)。 Kubernetes Dashboard 主要关注 Kubernetes 的内建资源类型，如 Pods、Services、Deployments 等。

对于CRDs的管理，你可能需要使用其他的Kubernetes管理界面工具，如 Lens、Rancher、OpenShift等，它们支持更广泛的Kubernetes功能，包括CRDs。